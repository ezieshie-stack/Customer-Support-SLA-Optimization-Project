{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Executive Brief: Support Operations & SLA Optimization\n",
                "**Prepared By**: Senior Data Analyst\n",
                "\n",
                "## 1. The Business Problem\n",
                "Our Support Operations team is facing challenges with inconsistent resolution times and missed SLAs. To address this, we have initiated a comprehensive audit of our ticket data to answer:\n",
                "1. **Where are we failing?** (Descriptive Analytics)\n",
                "2. **Why are we failing?** (Statistical & Root Cause Analysis)\n",
                "3. **How can we fix it?** (Predictive Modeling & Strategic Recommendations)\n",
                "\n",
                "### Core KPIs Audited\n",
                "- **SLA Breach Rate**: Target < 10% for Critical Tickets.\n",
                "- **Resolution Time**: Identifying barriers to speed.\n",
                "- **Financial Risk**: Quantifying the cost of service failures."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Setup & Imports\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import plotly.express as px\n",
                "import plotly.graph_objects as go\n",
                "from scipy.stats import chi2_contingency, ttest_ind, mannwhitneyu, norm\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
                "from sklearn.cluster import KMeans\n",
                "\n",
                "# Settings for cleaner output\n",
                "pd.set_option('display.max_columns', None)\n",
                "sns.set_style(\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print(\"Libraries loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Data\n",
                "Ingesting the raw ticket logs for analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dataset\n",
                "try:\n",
                "    df = pd.read_csv('../data/customer_support_tickets.csv')\n",
                "    print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
                "except FileNotFoundError:\n",
                "    # Fallback for different working directories\n",
                "    try:\n",
                "        df = pd.read_csv('customer_support_tickets.csv')\n",
                "        print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
                "    except FileNotFoundError:\n",
                "        print(\"Error: customer_support_tickets.csv not found.\")\n",
                "\n",
                "if 'df' in locals():\n",
                "    display(df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. SLA Definition & Business Logic (Canonical)\n",
                "**SINGLE SOURCE OF TRUTH**\n",
                "Here we define exactly what constitutes a \"Breach\" and the financial cost associated with it.\n",
                "Any downstream analysis MUST use `Resolution_Hours` and `Is_SLA_Breach` defined here.\n",
                "\n",
                "**Logic Rules**:\n",
                "1. **Ticket Creation**: Imputed (1-5h before first response) due to missing raw log.\n",
                "2. **Resolution Hours**: `Time Resolved` - `Creation Time`.\n",
                "3. **SLA Targets**: Critical (4h), High (8h), Normal (24h), Low (72h)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- CANONICAL SLA LOGIC ENGINE ---\n",
                "\n",
                "# A. Date Conversion\n",
                "df['Time_Resolved'] = pd.to_datetime(df['Time to Resolution'], errors='coerce')\n",
                "df['Time_First_Response'] = pd.to_datetime(df['First Response Time'], errors='coerce')\n",
                "\n",
                "# B. Filter Valid Rows\n",
                "df_sla = df.dropna(subset=['Time_Resolved', 'Time_First_Response']).copy()\n",
                "\n",
                "# C. Impute Creation Date (Simulation of Ground Truth)\n",
                "np.random.seed(42)\n",
                "random_hours = pd.to_timedelta(np.random.randint(1, 6, size=len(df_sla)), unit='h')\n",
                "df_sla['Ticket Creation Date'] = df_sla['Time_First_Response'] - random_hours\n",
                "\n",
                "# D. Calculate Resolution Hours\n",
                "df_sla['Resolution_Hours'] = (df_sla['Time_Resolved'] - df_sla['Ticket Creation Date']).dt.total_seconds() / 3600\n",
                "df_sla = df_sla[df_sla['Resolution_Hours'] > 0].copy() # Filter hygiene\n",
                "\n",
                "# E. Define SLA Targets\n",
                "def get_sla_target(priority):\n",
                "    targets = {'Critical': 4, 'High': 8, 'Normal': 24, 'Low': 72}\n",
                "    return targets.get(priority, 24)\n",
                "\n",
                "df_sla['SLA_Target_Hours'] = df_sla['Ticket Priority'].apply(get_sla_target)\n",
                "\n",
                "# F. Determine Breach Status\n",
                "df_sla['Is_SLA_Breach'] = df_sla['Resolution_Hours'] > df_sla['SLA_Target_Hours']\n",
                "df_sla['Is_SLA_Breach_Numeric'] = df_sla['Is_SLA_Breach'].astype(int)\n",
                "\n",
                "# G. Assign Financial Risk (Cost Logic)\n",
                "def get_breach_cost(row):\n",
                "    if not row['Is_SLA_Breach']: return 0\n",
                "    # Cost = Penalty + Churn Risk Estimate\n",
                "    costs = {'Critical': 500, 'High': 200, 'Normal': 50, 'Low': 10}\n",
                "    return costs.get(row['Ticket Priority'], 0)\n",
                "\n",
                "df_sla['Est_Breach_Cost'] = df_sla.apply(get_breach_cost, axis=1)\n",
                "\n",
                "# Extract Hour for Workload Analyis\n",
                "df_sla['Hour_of_Day'] = df_sla['Ticket Creation Date'].dt.hour\n",
                "\n",
                "print(\"âœ… SLA Logic & Financial Risk Engine Applied.\")\n",
                "print(f\"Analyzable Dataset: {df_sla.shape[0]} tickets.\")\n",
                "display(df_sla[['Ticket Creation Date', 'Resolution_Hours', 'SLA_Target_Hours', 'Is_SLA_Breach', 'Est_Breach_Cost']].head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 2A: Descriptive Risk Analytics\n",
                "**Goal**: Identify \"Where are we bleeding?\"\n",
                "Visualizing the operational landscape to pinpoint the bleeding."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 6))\n",
                "sns.barplot(x='Ticket Priority', y='Is_SLA_Breach', data=df_sla, order=['Critical', 'High', 'Normal', 'Low'], ci=None, palette='viridis')\n",
                "plt.title('SLA Breach Rate by Priority')\n",
                "plt.ylabel('Breach Rate')\n",
                "plt.axhline(df_sla['Is_SLA_Breach'].mean(), color='red', linestyle='--', label='Overall Average')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(12, 6))\n",
                "sns.histplot(data=df_sla, x='Resolution_Hours', hue='Ticket Priority', bins=50, kde=True, palette='viridis')\n",
                "plt.title('Distribution of Resolution Time by Priority')\n",
                "plt.xlim(0, 100) # Zoom in for readability, adjust as needed\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 2B: Diagnostic Root Cause Analysis\n",
                "**Goal**: Explain \"Why is this happening?\"\n",
                "We investigate Volume vs Risk, Channel Friction, and Product Complexity."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 2B.1 Volume vs Risk Analysis ---\n",
                "diagnostic_volume = (\n",
                "    df_sla.groupby(['Ticket Type', 'Ticket Priority'])\n",
                "      .agg(\n",
                "          Tickets=('Ticket ID', 'count'),\n",
                "          Breach_Rate=('Is_SLA_Breach_Numeric', 'mean'),\n",
                "          Avg_Resolution_Hours=('Resolution_Hours', 'mean'),\n",
                "          Total_Cost=('Est_Breach_Cost', 'sum')\n",
                "      )\n",
                "      .reset_index()\n",
                "      .sort_values('Total_Cost', ascending=False)\n",
                ")\n",
                "\n",
                "print(\"--- Volume vs Risk: Top Drivers ---\")\n",
                "display(diagnostic_volume.head(10))\n",
                "\n",
                "# Interpretation: High Cost usually comes from High Volume * High Breach Rate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 2B.2 Channel Friction Analysis ---\n",
                "channel_diagnostics = (\n",
                "    df_sla.groupby(['Ticket Channel'])\n",
                "      .agg(\n",
                "          Tickets=('Ticket ID', 'count'),\n",
                "          Breach_Rate=('Is_SLA_Breach_Numeric', 'mean'),\n",
                "          Avg_Resolution=('Resolution_Hours', 'mean'),\n",
                "          Avg_Cost=('Est_Breach_Cost', 'mean')\n",
                "      )\n",
                "      .reset_index()\n",
                "      .sort_values('Breach_Rate', ascending=False)\n",
                ")\n",
                "\n",
                "print(\"--- Channel Friction Analysis ---\")\n",
                "display(channel_diagnostics)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 2B.3 Product Complexity Signal ---\n",
                "product_diagnostics = (\n",
                "    df_sla.groupby(['Product Purchased'])\n",
                "      .agg(\n",
                "          Tickets=('Ticket ID', 'count'),\n",
                "          Breach_Rate=('Is_SLA_Breach_Numeric', 'mean'),\n",
                "          Avg_Resolution=('Resolution_Hours', 'mean'),\n",
                "          Total_Cost=('Est_Breach_Cost', 'sum')\n",
                "      )\n",
                "      .query('Tickets >= 10')\n",
                "      .sort_values('Breach_Rate', ascending=False)\n",
                ")\n",
                "\n",
                "print(\"--- Product Complexity: Top Failure Rates ---\")\n",
                "display(product_diagnostics.head(10))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 2B.4 Statistical Validation (Chi-Square) ---\n",
                "print(\"--- Chi-Square Test: Priority vs Breach ---\")\n",
                "contingency = pd.crosstab(df_sla['Ticket Priority'], df_sla['Is_SLA_Breach'])\n",
                "chi2, p_value, _, _ = chi2_contingency(contingency)\n",
                "\n",
                "print(f\"Chi-Square Statistic: {chi2:.4f}\")\n",
                "print(f\"P-Value: {p_value:.4e}\")\n",
                "\n",
                "if p_value < 0.05:\n",
                "    print(\"âœ… Result: Statistically Significant. Priority influences Breach Rate.\")\n",
                "else:\n",
                "    print(\"âŒ Result: Not Significant.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 2C: Predictive SLA Breach Modeling\n",
                "**Goal**: Predict \"Which incoming tickets will breach?\"\n",
                "We model risk at the moment of ticket creation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 2C.1 Feature Engineering ---\n",
                "model_features = [\n",
                "    'Ticket Priority',\n",
                "    'Ticket Channel',\n",
                "    'Ticket Type',\n",
                "    'Product Purchased',\n",
                "    'Hour_of_Day'\n",
                "]\n",
                "\n",
                "# Safety: Fill NaNs\n",
                "df_model = df_sla.copy().fillna('Unknown')\n",
                "\n",
                "X = pd.get_dummies(df_model[model_features], drop_first=True)\n",
                "y = df_model['Is_SLA_Breach_Numeric']\n",
                "\n",
                "print(f\"Feature Matrix Shape: {X.shape}\")\n",
                "print(f\"Baseline Breach Rate: {y.mean():.2%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 2C.2 Stratified Train/Test Split ---\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.25, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "print(f\"Training set: {X_train.shape[0]} samples\")\n",
                "print(f\"Test set: {X_test.shape[0]} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 2C.3 Baseline Model (Logistic Regression) ---\n",
                "model = LogisticRegression(\n",
                "    max_iter=1000, \n",
                "    class_weight='balanced', \n",
                "    solver='liblinear'\n",
                ")\n",
                "\n",
                "model.fit(X_train, y_train)\n",
                "print(\"âœ… Baseline Model Trained.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 2C.4 Model Evaluation (Business Lens) ---\n",
                "# Predictions\n",
                "y_pred = model.predict(X_test)\n",
                "y_prob = model.predict_proba(X_test)[:, 1]\n",
                "\n",
                "# Metrics\n",
                "print(\"--- Classification Report ---\")\n",
                "print(classification_report(y_test, y_pred))\n",
                "\n",
                "roc_auc = roc_auc_score(y_test, y_prob)\n",
                "print(f\"ROC-AUC Score: {roc_auc:.3f}\")\n",
                "\n",
                "# Financial Risk of False Negatives\n",
                "false_negatives = ((y_test == 1) & (y_pred == 0)).sum()\n",
                "estimated_loss = false_negatives * 50  # Conservative avg cost\n",
                "\n",
                "print(f\"\\nâš ï¸ False Negatives (Missed Breaches): {false_negatives}\")\n",
                "print(f\"ðŸ’¸ Est. Unmitigated Risk (Test Set): ${estimated_loss:,.0f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 2C.5 Feature Importance ---\n",
                "feature_importance = pd.DataFrame({\n",
                "    'Feature': X.columns,\n",
                "    'Coefficient': model.coef_[0]\n",
                "}).sort_values('Coefficient', ascending=False)\n",
                "\n",
                "print(\"--- Top Risk Factors (Positive Coef = Higher Risk) ---\")\n",
                "display(feature_importance.head(10))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 3: Optimization & Simulation\n",
                "**Goal**: Answer \"What operational change reduces breaches the most?\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 3A. Scenario-Ready Optimization Table ---\n",
                "opt_df = df_sla.copy()\n",
                "\n",
                "# Baseline metrics\n",
                "baseline = {\n",
                "    \"tickets\": len(opt_df),\n",
                "    \"breach_rate\": opt_df['Is_SLA_Breach'].mean(),\n",
                "    \"total_cost\": opt_df['Est_Breach_Cost'].sum()\n",
                "}\n",
                "print(\"Baseline State:\", baseline)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 3B. Identify High-Risk Segments ---\n",
                "segment = (\n",
                "    opt_df.groupby(['Ticket Priority', 'Ticket Channel'])\n",
                "    .agg(\n",
                "        Tickets=('Is_SLA_Breach', 'size'),\n",
                "        Breach_Rate=('Is_SLA_Breach', 'mean'),\n",
                "        Total_Cost=('Est_Breach_Cost', 'sum'),\n",
                "        Avg_Cost=('Est_Breach_Cost', 'mean')\n",
                "    )\n",
                "    .reset_index()\n",
                "    .sort_values('Total_Cost', ascending=False)\n",
                ")\n",
                "\n",
                "display(segment.head(10))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 3C. Define Scenario Function ---\n",
                "def simulate_intervention(df_in, segment_filter, reduction_rate=0.20):\n",
                "    \"\"\"\n",
                "    segment_filter: boolean mask on df\n",
                "    reduction_rate: percent reduction in breaches for targeted segment\n",
                "    \"\"\"\n",
                "    df_sim = df_in.copy()\n",
                "    \n",
                "    # baseline breaches in target segment\n",
                "    target_mask = segment_filter\n",
                "    target_breaches = df_sim.loc[target_mask, 'Is_SLA_Breach'].sum()\n",
                "    \n",
                "    # expected breaches avoided\n",
                "    avoided = int(round(target_breaches * reduction_rate))\n",
                "    \n",
                "    # simulate: mark some breaches as \"prevented\"\n",
                "    breach_idx = df_sim.loc[target_mask & (df_sim['Is_SLA_Breach'] == 1)].index\n",
                "    \n",
                "    # only flip as many as we \"avoid\"\n",
                "    if len(breach_idx) > 0:\n",
                "        flip_idx = breach_idx[:avoided]\n",
                "        df_sim.loc[flip_idx, 'Is_SLA_Breach'] = False\n",
                "        df_sim.loc[flip_idx, 'Est_Breach_Cost'] = 0\n",
                "    \n",
                "    results = {\n",
                "        \"breaches_avoided\": avoided,\n",
                "        \"new_breach_rate\": df_sim['Is_SLA_Breach'].mean(),\n",
                "        \"new_total_cost\": df_sim['Est_Breach_Cost'].sum(),\n",
                "        \"cost_saved\": df_in['Est_Breach_Cost'].sum() - df_sim['Est_Breach_Cost'].sum()\n",
                "    }\n",
                "    return results\n",
                "\n",
                "print(\"âœ… Simulation Engine Ready.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 3D. Run 3 Key Scenarios ---\n",
                "\n",
                "# Scenario 1: Reduce breaches for Critical + Chat by 20%\n",
                "scenario1 = simulate_intervention(\n",
                "    opt_df,\n",
                "    segment_filter=(opt_df['Ticket Priority'] == 'Critical') & (opt_df['Ticket Channel'] == 'Chat'),\n",
                "    reduction_rate=0.20\n",
                ")\n",
                "\n",
                "# Scenario 2: Reduce breaches for High + Email by 15%\n",
                "scenario2 = simulate_intervention(\n",
                "    opt_df,\n",
                "    segment_filter=(opt_df['Ticket Priority'] == 'High') & (opt_df['Ticket Channel'] == 'Email'),\n",
                "    reduction_rate=0.15\n",
                ")\n",
                "\n",
                "# Scenario 3: Reduce breaches for Top 2 cost segments by 10%\n",
                "top2 = segment.head(2)[['Ticket Priority','Ticket Channel']].values.tolist()\n",
                "\n",
                "mask_top2 = pd.Series([False] * len(opt_df))\n",
                "for p, c in top2:\n",
                "    mask_top2 = mask_top2 | ((opt_df['Ticket Priority'] == p) & (opt_df['Ticket Channel'] == c))\n",
                "\n",
                "scenario3 = simulate_intervention(\n",
                "    opt_df,\n",
                "    segment_filter=mask_top2,\n",
                "    reduction_rate=0.10\n",
                ")\n",
                "\n",
                "print(\"Scenarios Run.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 3E. Scenario Comparison Table ---\n",
                "comparison = pd.DataFrame([\n",
                "    {\"Scenario\": \"Baseline\", **baseline, \"breaches_avoided\": 0, \"cost_saved\": 0},\n",
                "    {\"Scenario\": \"S1: Critical+Chat 20% reduction\", **scenario1},\n",
                "    {\"Scenario\": \"S2: High+Email 15% reduction\", **scenario2},\n",
                "    {\"Scenario\": \"S3: Top2 Segments 10% reduction\", **scenario3},\n",
                "])\n",
                "\n",
                "display(comparison)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- DASHBOARD EXPORT ---\n",
                "# Exports the rigorous data for the Streamlit App\n",
                "\n",
                "print(\"Exporting Dashboard Data...\")\n",
                "export_df = df_sla.copy()\n",
                "\n",
                "# Add ID if missing\n",
                "if \"Ticket ID\" not in export_df.columns:\n",
                "    export_df.insert(0, \"Ticket ID\", range(1, len(export_df) + 1))\n",
                "\n",
                "# Realistic Date Simulation (Jan-Mar 2023)\n",
                "np.random.seed(42)\n",
                "start_date = pd.to_datetime('2023-01-01')\n",
                "end_date = pd.to_datetime('2023-03-31')\n",
                "days_range = (end_date - start_date).days\n",
                "random_days = np.random.randint(0, days_range, size=len(export_df))\n",
                "export_df[\"Ticket_Date\"] = [start_date + pd.Timedelta(days=x) for x in random_days]\n",
                "export_df[\"Ticket_Date\"] = pd.to_datetime(export_df[\"Ticket_Date\"]).dt.date\n",
                "\n",
                "# Add Predictions from 2C Model (if available)\n",
                "if 'model' in locals():\n",
                "    # Re-encode full dataset for prediction\n",
                "    # Note: In production, use a Pipeline to handle this automatically\n",
                "    X_full = pd.get_dummies(export_df.fillna('Unknown')[model_features], drop_first=True)\n",
                "    # Align columns with training data\n",
                "    missing_cols = set(X.columns) - set(X_full.columns)\n",
                "    for c in missing_cols: X_full[c] = 0\n",
                "    X_full = X_full[X.columns] # Enforce order\n",
                "    \n",
                "    export_df['Pred_Breach_Prob'] = model.predict_proba(X_full)[:, 1]\n",
                "    export_df[\"Risk_Bucket\"] = pd.cut(\n",
                "        export_df[\"Pred_Breach_Prob\"],\n",
                "        bins=[0, 0.3, 0.6, 1.0],\n",
                "        labels=[\"Low Risk\", \"Medium Risk\", \"High Risk\"],\n",
                "        include_lowest=True\n",
                "    )\n",
                "else:\n",
                "    export_df[\"Pred_Breach_Prob\"] = 0\n",
                "    export_df[\"Risk_Bucket\"] = \"N/A\"\n",
                "\n",
                "export_df.rename(columns={\"Est_Breach_Cost\": \"Breach_Cost\"}, inplace=True)\n",
                "\n",
                "# Select Columns\n",
                "final_cols = ['Ticket ID', 'Ticket_Date', 'Ticket Priority', 'Ticket Channel', \n",
                "              'Ticket Type', 'Product Purchased', 'Resolution_Hours', \n",
                "              'Is_SLA_Breach', 'Breach_Cost', 'Pred_Breach_Prob', 'Risk_Bucket']\n",
                "              \n",
                "df_dashboard = export_df[final_cols].copy()\n",
                "out_path = \"../outputs/dashboard/customer_support_sla_dashboard.csv\"\n",
                "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
                "df_dashboard.to_csv(out_path, index=False)\n",
                "print(f\"âœ… Dashboard CSV Exported.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}